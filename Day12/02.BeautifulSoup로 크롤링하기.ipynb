{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "<p>웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam01.html\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p1 = 웹 페이지를 분석하는 것\n",
      "p2 = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 글자 출력하기\n",
    "print(\"h1 =\", h1.get_text()) # string, text, get_text()\n",
    "print(\"p1 =\", p1.get_text()) # string, text, get_text()\n",
    "print(\"p2 =\", p2.get_text()) # string, text, get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id로 찾는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "<p id=\"body\">웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam02.html\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : <h1 id=\"title\">스크레이핑이란?</h1>\n",
      "body : 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# find()로 원하는 부분 추출하기\n",
    "title = soup.find(id=\"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "print(\"title :\", title.string)\n",
    "print(\"body :\", body.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러개의 요소 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<ul>\n",
      "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
      "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam03.html\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for a in links:\n",
    "  text = a.string \n",
    "  href = a.attrs['href']\n",
    "  print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기상청 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (하늘상태) 이번 예보기간은 구름많은 날이 많겠으나, 19일(월)~21일(수)은 흐리겠습니다.<br />○ (기온) 아침 기온은 23~27도, 낮 기온은 31~34도로 평년(최저기온 21~24도, 최고기온 29~32도)보다 조금 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.<br /><br />* 이번 예보기간 동안 최고체감온도가 35도 내외로 올라 무더위와 열대야가 나타나는 날이 많겠고, 대기 불안정으로 인한 소나기가 내릴 가능성이 있겠으니, 앞으로 발표되는 최신 예보를 참고하기 바랍니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\kenny\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# 기상청 RSS 사이트 주소\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ (하늘상태) 이번 예보기간은 구름많은 날이 많겠으나, 19일(월)~21일(수)은 흐리겠습니다.\n",
      "\n",
      "○ (기온) 아침 기온은 23~27도, 낮 기온은 31~34도로 평년(최저기온 21~24도, 최고기온 29~32도)보다 조금 높겠습니다.\n",
      "\n",
      "○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n",
      "\n",
      "\n",
      "* 이번 예보기간 동안 최고체감온도가 35도 내외로 올라 무더위와 열대야가 나타나는 날이 많겠고, 대기 불안정으로 인한 소나기가 내릴 가능성이 있겠으니, 앞으로 발표되는 최신 예보를 참고하기 바랍니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = wf.replace(\"다.\", \"다.\\n\")\n",
    "texts = texts.split(\"<br />\")\n",
    "for t in texts:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS 선택자 사용하기\n",
    ": BeautifulSoup은 자바스크립트 라이브러리인 JQuery처럼 CSS선택자를 지정해서 원하는 요소를 추출이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"meigen\">\n",
      "<h1>위키북스 도서</h1>\n",
      "<ul class=\"items\">\n",
      "<li>유니티 게임 이펙트 입문</li>\n",
      "<li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
      "<li>모던 웹사이트 디자인의 정석</li>\n",
      "</ul>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam04.html\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1: 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# (# : id, .:class, >:자식, 빈칸: 후손)\n",
    "\n",
    "# 타이틀 부분 추출하기\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(\"h1:\", h1)\n",
    "\n",
    "# 목록 부분 여러개 추출하기\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "  print(\"li =\", li.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 금융에서 환율정보 추출하기\n",
    ": https://finance.naver.com/marketindex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw 1,366.00\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value\n",
    "price = soup.select_one(\"#exchangeList > li.on > a.head.usd > div > span.value\").string\n",
    "print(\"usd/krw :\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 IT뉴스\n",
    "https://media.daum.net/digital/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20240811120027009 : “제약업계, 고효능·저독성 유망 기술 ‘TPD’ 주목”\n",
      "https://v.daum.net/v/20240811113843739 : 리니지M, 구글 매출 1위 탈환…주간 모바일게임 순위는\n",
      "https://v.daum.net/v/20240811112811619 : 더존비즈온, KG그룹에 비즈니스 플랫폼 `옴니이솔` 공급\n",
      "https://v.daum.net/v/20240811100624443 : 우주정거장 갇힌 ‘보잉 우주인’… 나사 “스페이스X로 귀환 검토”\n",
      "https://v.daum.net/v/20240811105723148 : \"민감정보 유출 주의\"…마이크로소프트, 새 보안 취약점 공개\n",
      "https://v.daum.net/v/20240811105412105 : 페르세우스 유성우, 10~12일 절정\n",
      "https://v.daum.net/v/20240811090713655 : 엔비디아발 GPU 지연에 부각되는 `GPU 최적화 솔루션`?\n",
      "https://v.daum.net/v/20240811100812469 : 피차이 \"그가 없는 세상 상상 힘들어\"...`유튜브 주역` 수전 워치츠키 별세\n",
      "https://v.daum.net/v/20240811084433299 : 튀르키예, 아동 안전 문제로 로블록스 차단\n",
      "https://v.daum.net/v/20240811091016696 : LG U+ ‘사운드바 블랙2’ 써보니.. 영화관 온 것 같네 [1일IT템]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "\n",
    "# Site Address\n",
    "url = \"https://media.daum.net/digital/\"\n",
    "\n",
    "response = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "a_list = soup.select(\"body > div.container-doc.cont-category > main > section > div.main-sub > div.box_g.box_news_major > ul a\")\n",
    "link_list = []\n",
    "\n",
    "for a in a_list:\n",
    "  # print(a)\n",
    "  title = a.string\n",
    "  href = a.attrs['href']\n",
    "  link_list.append(href)\n",
    "  print(href, \":\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://v.daum.net/v/20240811120027009', 'https://v.daum.net/v/20240811113843739', 'https://v.daum.net/v/20240811112811619', 'https://v.daum.net/v/20240811100624443', 'https://v.daum.net/v/20240811105723148', 'https://v.daum.net/v/20240811105412105', 'https://v.daum.net/v/20240811090713655', 'https://v.daum.net/v/20240811100812469', 'https://v.daum.net/v/20240811084433299', 'https://v.daum.net/v/20240811091016696']\n"
     ]
    }
   ],
   "source": [
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
